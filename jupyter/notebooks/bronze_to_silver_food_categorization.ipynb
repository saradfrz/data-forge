{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ecc6bc7-da30-4298-a5d4-0e39108adc8d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "<div style=\"background-color: #222; padding: 24px;\">\n",
    "    <h1 style=\"color: #d4bbff; margin-bottom: 8px;\">Bronze to Silver: Food Item Categorization Pipeline</h1>\n",
    "    <h3 style=\"color: #fff; margin-top: 0;\">Categorize and export food items from bronze to gold layer.</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63aef5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully. Version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "env_path = os.path.join(os.getcwd(), '.env')\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Define paths with validation\n",
    "INPUT_DATA_PATH = \"/home/jovyan/data/bronze\"\n",
    "OUTPUT_DATA_PATH = \"/home/jovyan/data/gold\"\n",
    "\n",
    "# Verify directories exist and are accessible\n",
    "def verify_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        os.chmod(path, 0o777)  # RWX for all\n",
    "    if not os.access(path, os.R_OK | os.W_OK):\n",
    "        raise PermissionError(f\"Insufficient permissions for path: {path}\")\n",
    "\n",
    "try:\n",
    "    verify_directory(INPUT_DATA_PATH)\n",
    "    verify_directory(OUTPUT_DATA_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"Directory verification failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Configure Spark with enhanced settings\n",
    "spark_master = os.getenv(\"SPARK_MASTER\", \"spark://spark-master:7077\")  # Default fallback\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .set(\"spark.hadoop.fs.permissions.umask-mode\", \"000\") \\\n",
    "    .set(\"spark.sql.sources.ignoreNonExistentPaths\", \"true\") \\\n",
    "    .set(\"spark.executor.extraJavaOptions\", \"-Djava.io.tmpdir=/tmp\") \\\n",
    "    .set(\"spark.driver.extraJavaOptions\", \"-Djava.io.tmpdir=/tmp\") \\\n",
    "    .set(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "    .set(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
    "    .set(\"spark.executor.memory\", \"2g\") \\\n",
    "    .set(\"spark.driver.memory\", \"2g\") \\\n",
    "    .set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "\n",
    "# Initialize Spark with error handling\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName(\"DataProcessing\") \\\n",
    "        .master(spark_master) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Verify Spark connectivity\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(f\"Spark session created successfully. Version: {spark.version}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize Spark session: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9581b456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Check output at: /home/jovyan/data/gold/spark_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Hard-set path\n",
    "OUTPUT_DATA_PATH = \"/home/jovyan/data/gold/spark_output\"\n",
    "\n",
    "# Nuclear directory cleanup\n",
    "os.system(f\"rm -rf {OUTPUT_DATA_PATH}\")\n",
    "os.makedirs(OUTPUT_DATA_PATH, exist_ok=True)\n",
    "os.chmod(OUTPUT_DATA_PATH, 0o777)\n",
    "\n",
    "# Test write\n",
    "test_df = spark.createDataFrame([(1, \"test\")], [\"id\", \"value\"])\n",
    "test_df.coalesce(1).write \\\n",
    "    .option(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(OUTPUT_DATA_PATH)\n",
    "\n",
    "print(\"Success! Check output at:\", OUTPUT_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "223daf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "     \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b8d823b-c9af-4c20-8173-60fbd6eb5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, ArrayType\n",
    ")\n",
    "\n",
    "# Schema for each purchase item inside the purchase array\n",
    "purchase_item_schema = StructType([\n",
    "    StructField(\"Código\", StringType(), True),\n",
    "    StructField(\"Descrição\", StringType(), True),\n",
    "    StructField(\"Qtde\", StringType(), True),\n",
    "    StructField(\"Un\", StringType(), True),\n",
    "    StructField(\"Vl Unit\", StringType(), True),\n",
    "    StructField(\"Vl Total\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Root JSON schema\n",
    "json_schema = StructType([\n",
    "    StructField(\"store\", StringType(), True),\n",
    "    StructField(\"cnpj\", StringType(), True),\n",
    "    StructField(\"store_state_code\", StringType(), True),\n",
    "    StructField(\"store_address\", StringType(), True),\n",
    "    StructField(\"purchase_date\", StringType(), True),  # we can parse date after loading\n",
    "    StructField(\"access_key\", StringType(), True),\n",
    "    StructField(\"purchase\", ArrayType(purchase_item_schema), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b91ce9-433b-4e36-8ba6-36ce98d74496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store: string (nullable = true)\n",
      " |-- cnpj: string (nullable = true)\n",
      " |-- store_state_code: string (nullable = true)\n",
      " |-- store_address: string (nullable = true)\n",
      " |-- purchase_date: string (nullable = true)\n",
      " |-- access_key: string (nullable = true)\n",
      " |-- purchase: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Código: string (nullable = true)\n",
      " |    |    |-- Descrição: string (nullable = true)\n",
      " |    |    |-- Qtde: string (nullable = true)\n",
      " |    |    |-- Un: string (nullable = true)\n",
      " |    |    |-- Vl Unit: string (nullable = true)\n",
      " |    |    |-- Vl Total: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.schema(json_schema).json(INPUT_DATA_PATH, multiLine=True)\n",
    "df.printSchema()\n",
    "#df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0423bb6-8fb3-4f6e-824c-bdb144de586d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "677"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = df.inputFiles()\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2280fb1-3842-4564-913a-41b5d1938b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+----------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|               store|              cnpj|store_state_code|       store_address|      purchase_date|          access_key|            purchase|\n",
      "+--------------------+------------------+----------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|RIGHI COM.DE GEN....|89.897.201/0002-28|      1060042441|RUA JOAO PESSOA, ...|03/09/2024 09:18:50|4324 0989 8972 01...|[{195, BERINJELA ...|\n",
      "|RIGHI COM.DE GEN....|89.897.201/0002-28|      1060042441|RUA JOAO PESSOA, ...|08/07/2024 15:19:27|4324 0789 8972 01...|[{29, RUCULA UN, ...|\n",
      "+--------------------+------------------+----------------+--------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b81954-af01-4764-b519-d4ae81fe5249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+----+-------+--------+------------------------------+-------------------+\n",
      "|CD_ITEM|DS_ITEM             |QTDE |UN  |VL_UNIT|VL_TOTAL|STORE                         |PURCHASED_AT       |\n",
      "+-------+--------------------+-----+----+-------+--------+------------------------------+-------------------+\n",
      "|195    |BERINJELA KG        |0,605|KG  |7,98   |4,83    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|196    |CEBOLA KG           |0,775|KG  |4,45   |3,45    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|197    |CENOURA KG          |0,665|KG  |3,12   |2,07    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|223    |TOMATE LONGA VIDA KG|1,145|KG  |3,77   |4,32    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|257    |ABOBORA DE TRONCO KG|1,43 |KG  |12,6   |18,02   |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|726    |BERGAMOTA MONTEN.KG |0,65 |KG  |5,9    |3,84    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|285    |Q.LANCHE FAT KG     |0,562|KG  |54,5   |30,63   |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|378    |GUISADO DE PRIMEIRA |3,036|KG  |36,7   |111,42  |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|85042  |AMAC.DOWNY 1.5L     |1    |UNID|36,3   |36,30   |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|2267   |FERM.ROYAL PO 250G  |1    |UNID|9,65   |9,65    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "+-------+--------------------+-----+----+-------+--------+------------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col, to_timestamp\n",
    "\n",
    "df_exploded = df.select(\n",
    "    \"store\",\n",
    "    to_timestamp(col(\"purchase_date\"), \"dd/MM/yyyy HH:mm:ss\").alias(\"purchased_at\"),\n",
    "    explode(\"purchase\").alias(\"purchase_item\")\n",
    ").select(\n",
    "    col(\"purchase_item.Código\").alias(\"CD_ITEM\"),\n",
    "    col(\"purchase_item.Descrição\").alias(\"DS_ITEM\"),\n",
    "    col(\"purchase_item.Qtde\").alias(\"QTDE\"),\n",
    "    col(\"purchase_item.Un\").alias(\"UN\"),\n",
    "    col(\"purchase_item.`Vl Unit`\").alias(\"VL_UNIT\"),\n",
    "    col(\"purchase_item.`Vl Total`\").alias(\"VL_TOTAL\"),\n",
    "    \"STORE\",\n",
    "    \"PURCHASED_AT\"\n",
    ")\n",
    "\n",
    "df_exploded.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf06d39-fae0-444b-9cf9-00e6d710edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def clean_decimal_columns(df, column_names):\n",
    "    \"\"\"\n",
    "    Replace ',' with '.' in specified string columns and cast them to DoubleType.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The Spark DataFrame.\n",
    "        column_names (list): List of column names (strings) to clean.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The transformed DataFrame with cleaned float columns.\n",
    "    \"\"\"\n",
    "    for col_name in column_names:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            regexp_replace(col(col_name), \",\", \".\").cast(DoubleType())\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fcccba7-e374-461d-a722-dfdc3fd8b969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CD_ITEM: string (nullable = true)\n",
      " |-- DS_ITEM: string (nullable = true)\n",
      " |-- QTDE: double (nullable = true)\n",
      " |-- UN: string (nullable = true)\n",
      " |-- VL_UNIT: double (nullable = true)\n",
      " |-- VL_TOTAL: double (nullable = true)\n",
      " |-- STORE: string (nullable = true)\n",
      " |-- PURCHASED_AT: timestamp (nullable = true)\n",
      "\n",
      "+-------+--------------------+-----+---+-------+--------+------------------------------+-------------------+\n",
      "|CD_ITEM|DS_ITEM             |QTDE |UN |VL_UNIT|VL_TOTAL|STORE                         |PURCHASED_AT       |\n",
      "+-------+--------------------+-----+---+-------+--------+------------------------------+-------------------+\n",
      "|195    |BERINJELA KG        |0.605|KG |7.98   |4.83    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|196    |CEBOLA KG           |0.775|KG |4.45   |3.45    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|197    |CENOURA KG          |0.665|KG |3.12   |2.07    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|223    |TOMATE LONGA VIDA KG|1.145|KG |3.77   |4.32    |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "|257    |ABOBORA DE TRONCO KG|1.43 |KG |12.6   |18.02   |RIGHI COM.DE GEN.ALIM.LTDA-F.1|2024-09-03 09:18:50|\n",
      "+-------+--------------------+-----+---+-------+--------+------------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_clean = [\"QTDE\", \"VL_UNIT\", \"VL_TOTAL\"]\n",
    "df_cleaned = clean_decimal_columns(df_exploded, columns_to_clean)\n",
    "\n",
    "df_cleaned.printSchema()\n",
    "df_cleaned.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cd83087-a56f-4e6a-adb9-88e41b2b2ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 4364\n"
     ]
    }
   ],
   "source": [
    "row_count = df_cleaned.count()\n",
    "print(f\"Number of rows: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9b5e8-0399-49c3-a5b0-a6b40613bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [{\"item\": \"guisado\", \"loja\": \"RIGHI\"}, {\"item\": \"tomate\", \"loja\": \"RIGHI\"}]\n",
    "filters_df = spark.createDataFrame(filters)\n",
    "filters_df.show()\n",
    "\n",
    "filters_df.createOrReplaceTempView(\"FILTERS\")\n",
    "df_exploded.createOrReplaceTempView(\"PURCHASES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee3c7e-3228-453d-93bc-c6c6f6b720a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM PURCHASES\n",
    "JOIN FILTERS ON  LOWER(PURCHASES.DS_ITEM) LIKE LOWER(CONCAT('%', FILTERS.item, '%'))\n",
    "  AND LOWER(PURCHASES.store) LIKE LOWER(CONCAT('%', FILTERS.loja, '%'))\n",
    "\"\"\"\n",
    "\n",
    "myresult = spark.sql(query)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a7c3d7f-5ecd-47e7-9e42-66d21f6ec472",
   "metadata": {},
   "source": [
    "#myresult.explain(True)\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://silver-postgres:5432/data_forge_silver\"\n",
    "\n",
    "connection_props = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "df_cleaned.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"purchases\",\n",
    "    mode=\"overwrite\",  # Use \"append\" to not drop the table\n",
    "    properties=connection_props\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42a0a7-b78f-4858-86b4-f0b74cf40909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the \"purchases\" table\n",
    "df_purchases = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"purchases\",\n",
    "    properties=connection_props\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4318f72-a4c0-4ab3-8043-a46654f5816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myresult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534cce07-99f8-4f2a-be22-1a1bfaa0ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns_to_uppercase(df):\n",
    "    \"\"\"\n",
    "    Rename columns to UPPER_SNAKE_CASE English equivalents.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with original column names.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "    rename_map = {\n",
    "        \"Código\": \"CODE\",\n",
    "        \"Descrição\": \"DESCRIPTION\",\n",
    "        \"Qtde\": \"QUANTITY\",\n",
    "        \"Un\": \"UNIT\",\n",
    "        \"Vl Unit\": \"UNIT_PRICE\",\n",
    "        \"Vl Total\": \"TOTAL_PRICE\",\n",
    "        \"store\": \"STORE\",\n",
    "        \"purchased_at\": \"PURCHASED_AT\"\n",
    "    }\n",
    "\n",
    "    for original, renamed in rename_map.items():\n",
    "        df = df.withColumnRenamed(original, renamed)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c4ef0-66e0-43fe-9bbc-5788ac3aea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = rename_columns_to_uppercase(df_cleaned)\n",
    "df_renamed.printSchema()\n",
    "df_renamed.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae810b-db66-4dac-aec9-919aa4f10a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_description = df_renamed.select(\"DESCRIPTION\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2f1f4-fa14-49a8-a3e2-048537ead35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define keyword lists (all lowercase) for categorization\n",
    "vegetables = ['berinjela', 'cebola', 'cenoura', 'tomate', 'abobora', 'pepino', 'rucula', 'batata', 'alface', 'brocolis', 'repolho', 'beterraba', 'mandioquinha']\n",
    "meats = ['bife', 'file', 'coxinha', 'chuleta', 'bacon', 'patinho', 'peito', 'frango', 'carne', 'coxa', 'linguiça', 'guisado', 'atum']\n",
    "dairy = ['leite', 'iogurte', 'queijo', 'mussarela', 'nata', 'margarina', 'manteiga', 'q.lanche']\n",
    "beverages = ['coca-cola', 'agua', 'suco', 'cha', 'energi', 'monster', 'cafe', 'nescafe', 'vinho', 'cerveja', 'guarana','dolce gusto']\n",
    "seasonings = ['tempero', 'molho', 'sazon', 'mostarda', 'catchup', 'sal', 'oregano', 'paprica', 'chimichurri', 'cominho', 'coentro', 'maionese', 'ext.elefante', 'vinagre']\n",
    "grains = ['arroz', 'feijao', 'farinha', 'massa', 'pao', 'bolo', 'tapioca', 'milho', 'lentilha', 'aveia', 'grao', 'far.maria', 'feij.pto']\n",
    "snacks = ['biscoito', 'chocolate', 'snickers', 'bombom', 'paodequeijo', 'gelatina', 'dulce', 'doce', 'barra', 'cookie', 'pipoca']\n",
    "fruits = ['banana', 'laranja', 'maca', 'abacaxi', 'pera', 'uva', 'mamão', 'goiaba', 'manga', 'kiwi', 'ameixa', 'bergamota', 'tangerina', 'caqui', 'caju', 'morango']\n",
    "\n",
    "non_food_keywords = ['amac.downy', 'esponja', 'papel hig', 'det.liq', 'sab liq', 'limpador', 'colgate', 'vela', 'toalha', 'algodao', 'abs', 'antartica', 'isquiero', 'suporte', 'coador', 'pinça', 'lixa', 'alicate', 'cortador', 'perfume', 'desodorante', 'sabonete', 'pente', 'escova', 'gel', 'repelente', 'pasta', 'creme', 'shampoo', 'condicionador']\n",
    "\n",
    "def categorize(description):\n",
    "    if description is None:\n",
    "        return (\"No\", \"Non-food\")\n",
    "    \n",
    "    desc = description.lower()\n",
    "    \n",
    "    if any(k in desc for k in non_food_keywords):\n",
    "        return (\"No\", \"Non-food\")\n",
    "    if any(k in desc for k in vegetables):\n",
    "        return (\"Yes\", \"Vegetable\")\n",
    "    if any(k in desc for k in meats):\n",
    "        return (\"Yes\", \"Meat\")\n",
    "    if any(k in desc for k in dairy):\n",
    "        return (\"Yes\", \"Dairy\")\n",
    "    if any(k in desc for k in beverages):\n",
    "        return (\"Yes\", \"Beverage\")\n",
    "    if any(k in desc for k in seasonings):\n",
    "        return (\"Yes\", \"Seasoning\")\n",
    "    if any(k in desc for k in grains):\n",
    "        return (\"Yes\", \"Grain\")\n",
    "    if any(k in desc for k in snacks):\n",
    "        return (\"Yes\", \"Snack\")\n",
    "    if any(k in desc for k in fruits):\n",
    "        return (\"Yes\", \"Fruit\")\n",
    "    \n",
    "    return (\"No\", \"Non-food\")\n",
    "\n",
    "# Register UDF for Spark\n",
    "categorize_udf = F.udf(categorize, returnType=F.StructType().add(\"IS_FOOD\", StringType()).add(\"FOOD_TYPE\", StringType()))\n",
    "\n",
    "# Apply UDF and expand struct to columns\n",
    "df_categorized = df_description.withColumn(\"category\", categorize_udf(F.col(\"DESCRIPTION\"))) \\\n",
    "    .withColumn(\"IS_FOOD\", F.col(\"category.IS_FOOD\")) \\\n",
    "    .withColumn(\"FOOD_TYPE\", F.col(\"category.FOOD_TYPE\")) \\\n",
    "    .drop(\"category\")\n",
    "\n",
    "# Show sample\n",
    "df_categorized.show(10, truncate=False)\n",
    "\n",
    "# Filter unknown items for logging (not food)\n",
    "df_unknown = df_categorized.filter((F.col(\"IS_FOOD\") == \"No\") & (F.col(\"FOOD_TYPE\") == \"Non-food\"))\n",
    "\n",
    "# Collect unknown descriptions to driver and save log file\n",
    "unknown_list = df_unknown.select(\"DESCRIPTION\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9561c-d239-4c4d-9785-2d9df61df0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the categorized DataFrame as CSV\n",
    "\n",
    "df_categorized.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(OUTPUT_DATA_PATH)\n",
    "print(f\"Categorized CSV saved to: {OUTPUT_DATA_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
